{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "-O_OmwUHlPHG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDEsS3cjgPxi",
        "outputId": "d691d7b5-79cb-413d-c1d3-8d2440cbec7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape: (772, 31)\n",
            "Class\n",
            "0    763\n",
            "1      9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After balancing: (1526, 31)\n",
            "Class\n",
            "0    763\n",
            "1    763\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample size: 152\n",
            "\n",
            "\n",
            "Accuracy Results:\n",
            "    Sampling1  Sampling2  Sampling3  Sampling4  Sampling5\n",
            "M1      86.96      80.43      91.30      86.67      93.48\n",
            "M2      93.48     100.00      93.48      91.11      86.96\n",
            "M3      97.83     100.00     100.00      93.33      95.65\n",
            "M4      86.96      80.43      91.30      91.11      95.65\n",
            "M5      73.91      82.61      78.26      73.33      97.83\n",
            "\n",
            "Best sampling for each model:\n",
            "M1: Sampling5 (93.48%)\n",
            "M2: Sampling2 (100.00%)\n",
            "M3: Sampling2 (100.00%)\n",
            "M4: Sampling5 (95.65%)\n",
            "M5: Sampling5 (97.83%)\n",
            "\n",
            "Saved to results.csv\n"
          ]
        }
      ],
      "source": [
        "# read data\n",
        "df = pd.read_csv('/content/Creditcard_data.csv')\n",
        "\n",
        "print(\"Original data shape:\", df.shape)\n",
        "print(df['Class'].value_counts())\n",
        "\n",
        "# seperate x and y\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# oversample minority class\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_bal, y_bal = ros.fit_resample(X, y)\n",
        "\n",
        "# create df\n",
        "bal_df = pd.DataFrame(X_bal)\n",
        "bal_df.columns = X.columns\n",
        "bal_df['Class'] = y_bal\n",
        "\n",
        "print(\"\\nAfter balancing:\", bal_df.shape)\n",
        "print(bal_df['Class'].value_counts())\n",
        "\n",
        "# calc sample size\n",
        "total = len(bal_df)\n",
        "n = int(total * 0.1)\n",
        "print(f\"\\nSample size: {n}\")\n",
        "test_sz = 0.3\n",
        "\n",
        "# random sampling\n",
        "def random_sample(df, size):\n",
        "    samp = df.sample(n=size, random_state=42)\n",
        "    return samp\n",
        "\n",
        "# bootstrap\n",
        "def bootstrap_sample(df, size):\n",
        "    boot = df.sample(n=size, replace=True, random_state=123)\n",
        "    return boot\n",
        "\n",
        "# systematic\n",
        "def systematic_sample(dataframe, sample_size):\n",
        "    k = len(dataframe) // sample_size\n",
        "    idx = []\n",
        "\n",
        "    # tried using np.arange but this worked better\n",
        "    for i in range(0, len(dataframe), k):\n",
        "        idx.append(i)\n",
        "\n",
        "    if len(idx) > sample_size:\n",
        "        idx = idx[:sample_size]\n",
        "\n",
        "    result = dataframe.iloc[idx]\n",
        "    result = result.reset_index(drop=True)\n",
        "    return result\n",
        "\n",
        "# stratified\n",
        "def stratified_sample(df, size):\n",
        "    frac = size / len(df)\n",
        "\n",
        "    # keeps class ratio\n",
        "    strat = df.groupby('Class', group_keys=False).apply(\n",
        "        lambda grp: grp.sample(frac=frac, random_state=99)\n",
        "    )\n",
        "\n",
        "    strat = strat.reset_index(drop=True)\n",
        "    return strat\n",
        "\n",
        "# cluster based\n",
        "def cluster_sample(data, n_samples):\n",
        "    np.random.seed(55)\n",
        "\n",
        "    n_clusters = 10\n",
        "    per_clust = n_samples // n_clusters\n",
        "\n",
        "    # shuffle data\n",
        "    shuf = data.sample(frac=1, random_state=77)\n",
        "    shuf = shuf.reset_index(drop=True)\n",
        "\n",
        "    clust_len = len(shuf) // n_clusters\n",
        "\n",
        "    all_samp = []\n",
        "\n",
        "    # sample from each cluster\n",
        "    for i in range(n_clusters):\n",
        "        start = i * clust_len\n",
        "        end = start + clust_len\n",
        "\n",
        "        clust_data = shuf.iloc[start:end]\n",
        "\n",
        "        n_take = per_clust\n",
        "        if len(clust_data) < n_take:\n",
        "            n_take = len(clust_data)\n",
        "\n",
        "        samp = clust_data.sample(n=n_take, random_state=88)\n",
        "\n",
        "        all_samp.append(samp)\n",
        "        # print(f\"cluster {i}: {len(samp)}\")\n",
        "\n",
        "    # combine all\n",
        "    combined = pd.concat(all_samp, ignore_index=True)\n",
        "    return combined\n",
        "\n",
        "# create samples\n",
        "s1 = random_sample(bal_df, n)\n",
        "\n",
        "s2 = systematic_sample(bal_df, n)\n",
        "\n",
        "s3 = stratified_sample(bal_df, n)\n",
        "\n",
        "s4 = cluster_sample(bal_df, n)\n",
        "\n",
        "s5 = bootstrap_sample(bal_df, n)\n",
        "\n",
        "\n",
        "samps = [s1, s2, s3, s4, s5]\n",
        "samp_names = ['Sampling1', 'Sampling2', 'Sampling3', 'Sampling4', 'Sampling5']\n",
        "# print(f\"created {len(samps)} samples\")\n",
        "\n",
        "# models\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=123)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=456)\n",
        "\n",
        "sv = SVC(kernel='linear', random_state=789)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "\n",
        "models = [lr, dt, rf, sv, knn]\n",
        "mod_names = ['M1', 'M2', 'M3', 'M4', 'M5']\n",
        "\n",
        "# store results\n",
        "results = []\n",
        "\n",
        "# train on each sample\n",
        "for s in samps:\n",
        "\n",
        "    # seperate features and target\n",
        "    X_s = s.drop('Class', axis=1)\n",
        "    y_s = s['Class']\n",
        "\n",
        "    # split\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X_s, y_s, test_size=test_sz, random_state=42)\n",
        "\n",
        "    samp_res = []\n",
        "\n",
        "    # train each model\n",
        "    for m in models:\n",
        "\n",
        "        # train\n",
        "        m.fit(X_tr, y_tr)\n",
        "\n",
        "        # predict\n",
        "        preds = m.predict(X_te)\n",
        "\n",
        "        # get accuracy\n",
        "        acc = accuracy_score(y_te, preds)\n",
        "\n",
        "        # convert to percent\n",
        "        acc = acc * 100\n",
        "\n",
        "        samp_res.append(acc)\n",
        "        # print(f\"acc: {acc}\")\n",
        "\n",
        "    results.append(samp_res)\n",
        "\n",
        "# reorganize results\n",
        "res_dict = {}\n",
        "\n",
        "for i in range(len(mod_names)):\n",
        "\n",
        "    name = mod_names[i]\n",
        "\n",
        "    accs = []\n",
        "\n",
        "    for r in results:\n",
        "\n",
        "        accs.append(r[i])\n",
        "\n",
        "    res_dict[name] = accs\n",
        "\n",
        "# make df\n",
        "df_res = pd.DataFrame(res_dict)\n",
        "df_res.index = samp_names\n",
        "\n",
        "# transpose\n",
        "df_res = df_res.T\n",
        "\n",
        "print(\"\\n\\nAccuracy Results:\")\n",
        "print(df_res.round(2))\n",
        "\n",
        "print(\"\\nBest sampling for each model:\")\n",
        "\n",
        "for m in df_res.index:\n",
        "\n",
        "    best = df_res.loc[m].idxmax()\n",
        "\n",
        "    best_val = df_res.loc[m].max()\n",
        "\n",
        "    print(f\"{m}: {best} ({best_val:.2f}%)\")\n",
        "\n",
        "# save\n",
        "df_res.to_csv('results.csv')\n",
        "\n",
        "print(\"\\nSaved to results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These results show that different sampling techniques for different models give different accuracies.\n",
        "#For each model best performing sampling technique ko upar identify kiya gaya hai, which provides us with the maximum accuracy."
      ],
      "metadata": {
        "id": "ryG7uwTfke_n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}